
## HTTP 
HTTP stands for hypertext transfer protocol and is a protocol that defines a format for clients and servers to speak to each other.
A client sends an HTTP request and the server responds with an HTTP response.
It was intended to be a way of implementing a web of interconnecting computers to provide access to research and link them so they could easily reference one another in real time; a click of a link would open an associated document. 
HTTP is one many other protocols at the application layer of the OSI model that defines a standard form of data transfer from one source to another.
HTTP is just one of many other protocols used to transfer data at the application layer level.
HTTP is the foundation of data communication for the World Wide Web.
- protocol
A protocol is the special set of rules which specify interactions between the communicating entities.

# HTTP 0.9

From 1989, Tim Berners-Lee and his team at CERN , the European Nuclear Research Center in Switzerland, developed the Hypertext Transfer Protocol, together with the concepts of URL and HTML , which created the foundations of the World Wide Web. First results of this effort in 1991 were the version HTTP 0.9.

Methods supported: GET only
Response type: hypertext only
Connection nature: terminated immediately after the response
Headers: No HTTP headers (cannot transfer other content type files), No status/error codes, No URLs, No versioning
Support: HTTP/0.9 requests have been removed.
keep-alive: N/A

```javascript
Request
GET /
Response
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>301 Moved Permanently</title>
</head><body>
<h1>Moved Permanently</h1>
<p>The document has moved <a href="http://any23.apache.org">here</a>.</p>
</body></html>
```

## HTTP 1.0

In 1996, RFC 1945 became known as HTTP / 1.0. With HTTP / 1.0, a new TCP connection is established before each request and, after the response has been transmitted.

Methods supported: GET , HEAD , POST
Response type: not limited to hypertext (Content-Type header provided ability to transmit files other than plain HTML files — e.g. scripts, stylesheets, media)
Connection nature: terminated immediately after the response
Headers: does not officially required
Support: used by proxies, wget and some IE browsers.
keep-alive: not considered persistent unless a keep-alive header is included.
status code: only define up to 16 status codes.

so we can see HTTP 0.9 doesn’t support headers and HTTP 1.0 supports headers (not officially required).

## Major Problem in both HTTP/0.9 and HTTP/1.0

both are required to open new TCP connection per outstanding Request and it closes it immediately after the response was sent.

Short-lived connections
The original model of HTTP, and the default one in HTTP/1.0, is short-lived connections. Each HTTP request is completed on its own connection; this means a TCP handshake happens before each HTTP request, and these are serialized.

The TCP handshake itself is time-consuming, but a TCP connection adapts to its load, becoming more efficient with more sustained (or warm) connections. Short-lived connections do not make use of this efficiency feature of TCP, and performance degrades from optimum by persisting to transmit over a new, cold connection.

This model is the default model used in HTTP/1.0 (if there is no Connection header, or if its value is set to close). In HTTP/1.1, this model is only used when the Connection header is sent with a value of close.


## HTTP 1.1


In 1999, RFC 2616 , reflects the HTTP / 1.1 standards, With HTTP / 1.1 a client can use an additional header entry ( keep-alive ) to express a desire not to disconnect in order to be able to use the connection again (persistent connection).

Methods supported: GET , HEAD , POST , PUT , DELETE , TRACE , OPTIONS
Connection nature: long-lived
Support: common use until today.
keep-alive: considered persistent unless declared otherwise.
status code: came with 24 status codes.

### 1- HTTP pipelining: 
is a technique in which multiple HTTP requests are sent on a single TCP connection.
HTTP pipelining is not enabled by default in modern browsers due to proxy servers, Head-of-line blocking (HOL).
Each HTTP request over the TCP connection may be made immediately without waiting for the previous request's response to return. The responses will come back in the same order.

## 2- HTTP persistent connection (HTTP keep-alive)

is the idea of using a single TCP connection to send and receive multiple HTTP requests/responses, as opposed to opening a new connection for every single request/response pair.

Short-lived connections have two major hitches: the time taken to establish a new connection is significant, and performance of the underlying TCP connection gets better only when this connection has been in use for some time (warm connection). To ease these problems, the concept of a persistent connection has been designed, even prior to HTTP/1.1. Alternatively this may be called a keep-alive connection.

A persistent connection is one which remains open for a period of time, and can be reused for several requests, saving the need for a new TCP handshake, and utilizing TCP's performance enhancing capabilities. This connection will not stay open forever: idle connections are closed after some time (a server may use the Keep-Alive header to specify a minimum time the connection should be kept open).

Persistent connections also have drawbacks; even when idling they consume server resources, and under heavy load, DoS attacks can be conducted. In such cases, using non-persistent connections, which are closed as soon as they are idle, can provide better performance.

HTTP/1.0 connections are not persistent by default. Setting Connection to anything other than close, usually retry-after, will make them persistent.

In HTTP/1.1, persistence is the default, and the header is no longer needed (but it is often added as a defensive measure against cases requiring a fallback to HTTP/1.0).

## Compression/Decompression (Content Encoding) : 
a capability that can be built into web servers and web clients to improve transfer speed and bandwidth utilization, The most common compression schemes include gzip and Deflate.

The web client advertises which compression schemes it supports by including a list of tokens in the HTTP request.
```javascript
GET /encrypted-area HTTP/1.1
Host: www.example.com
Accept-Encoding: gzip, deflate
```
If the server supports compression scheme the server will add a Content-Encoding or Transfer-Encoding field in the HTTP response

## The Performance Issues Of HTTP/1.1
### 1-Head-of-line blocking(HOL blocking)

It as in networking is a performance issue that occurs when a bunch of packets is blocked by the first (a large or slow ) packet in line . It can happen especially in input buffered network switches where out-of-order delivery of packets can occur (arrive out of order). A switch can be composed of buffered input ports, a switch fabric and buffered output ports. If first-in-first-out (FIFO) input buffers are used, only the oldest packet is available for forwarding.


### 2-The repetition of information sent for each request

Each of the requests repeats a lot of the same information. Each includes the same user-agent information and accepted content information, etc.

### 3-The additional round trips(3-Way Handshake) needed.

## SPDY

SPDY is a protocol developed by Google to increase the speed and efficiency of delivering web content. SPDY modifies parts of the HyperText Transfer Protocol (HTTP) to improve web performance.

### HTTP Problems that SPDY Solves

- Single request per connection: HTTP can only fetch one resource at a time, leading to delays and underutilized bandwidth. SPDY allows for many concurrent downloads.
- Client-initiated requests: HTTP requires the user to request content from a server before it can be delivered. With SPDY, the server can “push” data to the client without having to wait for a request, making it possible to load web content before it’s needed.
Redundant Headers: HTTP headers define the behavior of an HTTP transaction. In some cases, the same headers are repeated over the course of a session. SPDY removes unnecessary headers to reduce the amount of bandwidth required.
Uncompressed Data: Compression, which shrinks the size of data during delivery, is optional for HTTP. SPDY forces compression for all communications including headers.

## HTTP/2 — A protocol for greater performance
HTTP/2 was based largely on Google’s own protocol SPDY which had many of the same features found in HTTP/2 and managed to improve data transmission while keeping backwards compatibility. SPDY had already proven many of the concepts used in HTTP/2.

- 1- Binary, instead of textual

Binary protocols are more efficient to parse, more compact “on the wire”, and most importantly, they are much less error-prone, compared to textual protocols like HTTP/1.x, because they often have a number of affordances to “help” with things like whitespace handling, capitalization, line endings, blank lines and so on.

- 2- Streams

an independent, bi-directional sequence of frames exchanged between the client and server within an HTTP/2 connection. Streams have several important characteristics:

A single HTTP/2 connection can contain multiple concurrently open streams, with either endpoint interleaving frames from multiple streams.
Streams can be established and used unilaterally(միակոմանիորեն) or shared by either the client or server.
Streams can be closed by either endpoint.
The order in which frames are sent on a stream is significant. Recipients process frames in the order they are received. In particular, the order of HEADERS and DATA frames is semantically significant.
Streams are identified by an integer. Stream identifiers are assigned to streams by the endpoint initiating the stream.

- 3- Request and Response Multiplexing

is a method in HTTP/2 by which multiple HTTP requests can be sent and responses can be received asynchronously via a single TCP connection. Multiplexing is the heart of HTTP/2 protocol.

With HTTP/1.x, if the client wants to make multiple parallel requests to improve performance, then multiple TCP connections must be used; see Using Multiple TCP Connections. This behaviour is a direct consequence of the HTTP/1.x delivery model, which ensures that only one response can be delivered at a time (response queuing) per connection. Worse, this also results in head-of-line blocking and inefficient use of the underlying TCP connection.

The new binary framing layer in HTTP/2 removes these limitations, and enables full request and response multiplexing, by allowing the client and server to break down an HTTP message into independent frames, interleave them, and then reassemble them on the other end.

- 4- Server Push

is the ability of the server to send multiple responses for a single client request. That is, in addition to the response to the original request, the server can push additional resources to the client, without the client having to request each one explicitly!

- HPACK- Data compression of HTTP headers

HTTP/2 introduces a form of header compression called HPACK. HPACK allows for very efficient header compression, without being vulnerable to compression related attacks such as CRIME. HPACK provides the following:

The ability to encode large headers using a fixed Huffman encoding
The ability to encode commonly used headers as a variable length integer, rather than re-sending the whole header each time.

## HTTP3

The main issue with TCP is that before establishing a session between a client and the server, a TLS handshake is needed to verify for a secure session.
HTTP/2 was unable to solve the issue of latency in lossy and slow connections. To address this, QUIC provides a native multiplexing and the lost packets mainly impact the streams where data has been dropped rather than stalling the entire system.

### The similarities between HTTP/2 and HTTP/3 include:

Both protocols make use of the server push mechanisms.
They offer multiplexing which is made over a single connection via streams.
Both versions utilize header compression as HPACK and QPACK which are tied to packet order.

### Differences between HTTP/2 and HTTP/3

The major difference is that HTTP/3 is based on QUIC as a transport layer to handle streams while HTTP/2 uses TCP to handles streams in the HTTP layer.
HTTP/3 has a much quicker handshake to establish a secure session compared to HTTP/2 which achieves this using TCP and TLS.
Here is a TCP/TLS vs QUIC latency comparison:

### Differences between HTTP/2 and HTTP/3
The major difference is that HTTP/3 is based on QUIC as a transport layer to handle streams while HTTP/2 uses TCP to handles streams in the HTTP layer.
HTTP/3 has a much quicker handshake to establish a secure session compared to HTTP/2 which achieves this using TCP and TLS.
Here is a TCP/TLS vs QUIC latency comparison:
Lastly, HTTP/3 can only be done in a secure and encrypted manner, while the HTTP/2 version can be implemented without HTTPS.
